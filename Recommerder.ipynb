{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Recommerder.ipynb","provenance":[],"collapsed_sections":["owlE68KAzRVY"],"authorship_tag":"ABX9TyP2w07eYGiwvOIUzo7rB/1g"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A--XHWcJywhM","executionInfo":{"status":"ok","timestamp":1619949226681,"user_tz":-420,"elapsed":1431,"user":{"displayName":"Hieu Nguyen Quang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhoqAOJz5TvatPuhnWRDsscEFEBB0kylHyhpfL7=s64","userId":"03779775999485249683"}},"outputId":"d60c1223-07b2-48fd-8922-727f32dfc047"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"k3LragLbye-k","executionInfo":{"status":"ok","timestamp":1619949227098,"user_tz":-420,"elapsed":1840,"user":{"displayName":"Hieu Nguyen Quang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhoqAOJz5TvatPuhnWRDsscEFEBB0kylHyhpfL7=s64","userId":"03779775999485249683"}},"outputId":"d09d2ae6-166a-49e8-8651-22a4eecae120"},"source":["import os\n","os.chdir('/content/drive/My Drive/MAICOGroup/Recommender/test/')\n","os.getcwd()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/My Drive/MAICOGroup/Recommender/test2'"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"owlE68KAzRVY"},"source":["##Sumarization"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NANov3vbzOnu","executionInfo":{"status":"ok","timestamp":1619949227770,"user_tz":-420,"elapsed":2505,"user":{"displayName":"Hieu Nguyen Quang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhoqAOJz5TvatPuhnWRDsscEFEBB0kylHyhpfL7=s64","userId":"03779775999485249683"}},"outputId":"3f35abab-0387-4a13-ef9d-cfef6c14b6e9"},"source":["# Import libs & tools\n","import nltk\n","import os\n","import re\n","import math\n","import operator\n","\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import stopwords\n","from nltk.tokenize import sent_tokenize,word_tokenize\n","\n","nltk.download('punkt')\n","nltk.download(\"stopwords\")\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')\n","Stopwords = set(stopwords.words('english'))\n","wordlemmatizer = WordNetLemmatizer()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Y0zKQ3IO1PcP"},"source":["# remove special characters funtion (~!@#$%^&*()...)\n","def remove_special_characters(text):\n","    regex = r'[^a-zA-Z0-9\\s]'\n","    text = re.sub(regex,'',text)\n","    return text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bQ_bhgNQ1z8V"},"source":["# calculate freqency of words function\n","def freq(words):\n","    words = [word.lower() for word in words]\n","    dict_freq = {}\n","    words_unique = []\n","    for word in words:\n","       if word not in words_unique:\n","           words_unique.append(word)\n","    for word in words_unique:\n","       dict_freq[word] = words.count(word)\n","    return dict_freq"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TpsFO9TU2zH1"},"source":["# calculate score of sentense function\n","def sentence_importance(sentence,dict_freq,sentences):\n","     sentence_score = 0\n","     sentence = remove_special_characters(str(sentence)) \n","     sentence = re.sub(r'\\d+', '', sentence)\n","     pos_tagged_sentence = [] \n","     no_of_sentences = len(sentences)\n","     pos_tagged_sentence = pos_tagging(sentence)\n","     for word in pos_tagged_sentence:\n","          if word.lower() not in Stopwords and word not in Stopwords and len(word)>1: \n","                word = word.lower()\n","                word = wordlemmatizer.lemmatize(word)\n","                sentence_score = sentence_score + word_tfidf(dict_freq,word,sentences,sentence)\n","     return sentence_score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7eoOuFIr3jgU"},"source":["# pos tagging function\n","def pos_tagging(text):\n","    pos_tag = nltk.pos_tag(text.split())\n","    pos_tagged_noun_verb = []\n","    for word,tag in pos_tag:\n","        if tag == \"NN\" or tag == \"NNP\" or tag == \"NNS\" or tag == \"VB\" or tag == \"VBD\" or tag == \"VBG\" or tag == \"VBN\" or tag == \"VBP\" or tag == \"VBZ\":\n","             pos_tagged_noun_verb.append(word)\n","    return pos_tagged_noun_verb"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eOVZluIm4nSP"},"source":["# calculate TF score function\n","def tf_score(word,sentence):\n","    freq_sum = 0\n","    word_frequency_in_sentence = 0\n","    len_sentence = len(sentence)\n","    for word_in_sentence in sentence.split():\n","        if word == word_in_sentence:\n","            word_frequency_in_sentence = word_frequency_in_sentence + 1\n","    tf =  word_frequency_in_sentence/ len_sentence\n","    return tf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"od6vecoo4tLT"},"source":["# calculate IDF score function\n","def idf_score(no_of_sentences,word,sentences):\n","    no_of_sentence_containing_word = 0\n","    for sentence in sentences:\n","        sentence = remove_special_characters(str(sentence))\n","        sentence = re.sub(r'\\d+', '', sentence)\n","        sentence = sentence.split()\n","        sentence = [word for word in sentence if word.lower() not in Stopwords and len(word)>1]\n","        sentence = [word.lower() for word in sentence]\n","        sentence = [wordlemmatizer.lemmatize(word) for word in sentence]\n","        if word in sentence:\n","            no_of_sentence_containing_word = no_of_sentence_containing_word + 1\n","    idf = math.log10(no_of_sentences/no_of_sentence_containing_word)\n","    return idf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4W4UOSgD43ye"},"source":["# calculate TFIDF score function\n","def tf_idf_score(tf,idf):\n","    return tf*idf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nCQWYpW_33wR"},"source":["# calculate TFIDF function\n","def word_tfidf(dict_freq,word,sentences,sentence):\n","    word_tfidf = []\n","    tf = tf_score(word,sentence)\n","    idf = idf_score(len(sentences),word,sentences)\n","    tf_idf = tf_idf_score(tf,idf)\n","    return tf_idf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K6Q3COLu7-xw"},"source":["def lemmatize_words(words):\n","    lemmatized_words = []\n","    for word in words:\n","       lemmatized_words.append(wordlemmatizer.lemmatize(word))\n","    return lemmatized_words\n","def stem_words(words):\n","    stemmed_words = []\n","    for word in words:\n","       stemmed_words.append(stemmer.stem(word))\n","    return stemmed_words"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gNkKyPzr69yI"},"source":["# main function\n","def Summarization_text(file_path, Percentage):\n","  file = file_path\n","  file = open(file , 'r')\n","  text = file.read()\n","  tokenized_sentence = sent_tokenize(text)\n","  text = remove_special_characters(str(text))\n","  text = re.sub(r'\\d+', '', text)\n","  tokenized_words_with_stopwords = word_tokenize(text)\n","  tokenized_words = [word for word in tokenized_words_with_stopwords if word not in Stopwords]\n","  tokenized_words = [word for word in tokenized_words if len(word) > 1]\n","  tokenized_words = [word.lower() for word in tokenized_words]\n","  tokenized_words = lemmatize_words(tokenized_words)\n","  word_freq = freq(tokenized_words)\n","  # input_user = int(input('Percentage of information to retain(in percent):'))\n","  input_user = Percentage\n","  no_of_sentences = int((input_user * len(tokenized_sentence))/100)\n","  print(no_of_sentences)\n","  c = 1\n","  sentence_with_importance = {}\n","  for sent in tokenized_sentence:\n","      sentenceimp = sentence_importance(sent,word_freq,tokenized_sentence)\n","      sentence_with_importance[c] = sentenceimp\n","      c = c+1\n","  sentence_with_importance = sorted(sentence_with_importance.items(), key=operator.itemgetter(1),reverse=True)\n","  cnt = 0\n","  summary = []\n","  sentence_no = []\n","  for word_prob in sentence_with_importance:\n","      if cnt < no_of_sentences:\n","          sentence_no.append(word_prob[0])\n","          cnt = cnt+1\n","      else:\n","        break\n","  sentence_no.sort()\n","  cnt = 1\n","  for sentence in tokenized_sentence:\n","      if cnt in sentence_no:\n","        summary.append(sentence)\n","      cnt = cnt+1\n","  summary = \" \".join(summary)\n","  new_file = 'data_summaried/' + file_path.split('/')[1]\n","  outF = open(new_file,\"w\")\n","  outF.write(summary)\n","  print(\"\\n\")\n","  print(\"Summary:\")\n","  print(summary)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mZgsSbOgIGeu","executionInfo":{"status":"ok","timestamp":1619949230143,"user_tz":-420,"elapsed":4831,"user":{"displayName":"Hieu Nguyen Quang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhoqAOJz5TvatPuhnWRDsscEFEBB0kylHyhpfL7=s64","userId":"03779775999485249683"}},"outputId":"58108446-0533-410b-dc9e-ffb7f7403841"},"source":["# summarization with persentage=50%\n","text_path = 'data_raw/text1.txt'\n","Summarization_text(text_path, 50)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["7\n","\n","\n","Summary:\n","Women education is a catch all term which refers to the state of primary, secondary, tertiary and health education in girls and women. The stronger the roots are the bigger and stronger the tree will be spreading its branches; sheltering and protecting the needy. Women are the soul of a society; a society can well be judged by the way its women are treated. An educated man goes out to make the society better, while an educated woman; whether she goes out or stays at home, makes the house and its occupants better. Women play many roles in a society- mother, wife, sister, care taker, nurse etc. An educated mother will make sure that her children are educated, and will weigh the education of a girl child, same as boys. A society cannot at all progress if its women weep silently.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h4NS81NGMnRR","executionInfo":{"status":"ok","timestamp":1619949230688,"user_tz":-420,"elapsed":5367,"user":{"displayName":"Hieu Nguyen Quang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhoqAOJz5TvatPuhnWRDsscEFEBB0kylHyhpfL7=s64","userId":"03779775999485249683"}},"outputId":"d16a5a4a-d4dd-4b13-dd88-dc8aa0605879"},"source":["import glob\n","for file_path in glob.glob('data_raw/*.txt'):\n","  Summarization_text(file_path, 50)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["7\n","\n","\n","Summary:\n","Women education is a catch all term which refers to the state of primary, secondary, tertiary and health education in girls and women. The stronger the roots are the bigger and stronger the tree will be spreading its branches; sheltering and protecting the needy. Women are the soul of a society; a society can well be judged by the way its women are treated. An educated man goes out to make the society better, while an educated woman; whether she goes out or stays at home, makes the house and its occupants better. Women play many roles in a society- mother, wife, sister, care taker, nurse etc. An educated mother will make sure that her children are educated, and will weigh the education of a girl child, same as boys. A society cannot at all progress if its women weep silently.\n","2\n","\n","\n","Summary:\n","It is the second-most populous country, the seventh-largest country by land area, and the most populous democracy in the world. Bounded by the Indian Ocean on the south, the Arabian Sea on the southwest, and the Bay of Bengal on the southeast, it shares land borders with Pakistan to the west;[f] China, Nepal, and Bhutan to the north; and Bangladesh and Myanmar to the east.\n","3\n","\n","\n","Summary:\n","Narendra Damodardas Modi (/ˌnəˈreɪn.ðrə ˈðɑːˌmoʊ.ðər.ðɑːs ˈmoʊðiː/ (About this soundlisten) nə-RAIN-dhrə DHAH-moh-dhər-dhahss MO-dhee ; born 17 September 1950)[a] is an Indian politician serving as the 14th and current Prime Minister of India since 2014. He is also a member of the Rashtriya Swayamsevak Sangh (RSS), a Hindu nationalist volunteer organisation. He is the first prime minister born after India's independence, the second non-Congress one to win two consecutive terms after Atal Bihari Vajpayee and the first from outside the Congress to win both terms with a majority in the Lok Sabha.\n","2\n","\n","\n","Summary:\n","Rahul Gandhi (About this soundpronunciation (help·info) [ˈraːɦʊl ˈɡaːn̪d̪ʱi]) (born 19 June 1970) is an Indian politician and a member of the Indian Parliament, representing the constituency of Wayanad, Kerala in the 17th Lok Sabha. He is also a trustee of Rajiv Gandhi Foundation and Rajiv Gandhi Charitable Trust.\n","3\n","\n","\n","Summary:\n","The batting side scores runs by striking the ball bowled at the wicket with the bat (and running between the wickets), while the bowling and fielding side tries to prevent this (by preventing the ball from leaving the field, and getting the ball to either wicket) and dismiss each batter (so they are \"out\"). Means of dismissal include being bowled, when the ball hits the stumps and dislodges the bails, and by the fielding side either catching the ball after it is hit by the bat and before it hits the ground, or hitting a wicket with the ball before a batter can cross the crease in front of the wicket. The game is adjudicated by two umpires, aided by a third umpire and match referee in international matches.\n","3\n","\n","\n","Summary:\n","Sneakers (also called trainers, athletic shoes, tennis shoes, gym shoes, kicks, sport shoes, flats, running shoes, skate shoes, or runners) are shoes primarily designed for sports or other forms of physical exercise but that are now also widely used for everyday casual wear. Like other parts of the global clothing industry, manufacture of shoes is heavily concentrated in Asia with nine in ten shoes produced in that region. [4] About 90% of shoes end up in landfills at end of life.\n","3\n","\n","\n","Summary:\n","Apple Inc. is an American multinational technology company headquartered in Cupertino, California, that designs, develops, and sells consumer electronics, computer software, and online services. [8][9][10] Its hardware products include iPhone smartphones, iPad tablet computers, Mac personal computers, iPod portable media players, Apple Watch smartwatches, Apple TV digital media players, AirPods wireless earbuds, AirPods Max headphones, and the HomePod smart speaker line. Apple's software includes the iOS, iPadOS, macOS, watchOS, and tvOS operating systems, the iTunes media player, the Safari web browser, the Shazam music identifier, and the iLife and iWork creativity and productivity suites, as well as professional applications like Final Cut Pro, Logic Pro, and Xcode.\n","2\n","\n","\n","Summary:\n","Australia, officially the Commonwealth of Australia, is a sovereign country comprising the mainland of the Australian continent, the island of Tasmania, and numerous smaller islands. [14] Australia's capital is Canberra, and its largest city is Sydney.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uBAxvZ_qJ8k1"},"source":["##Recommendation"]},{"cell_type":"code","metadata":{"id":"UnFu5uzFP8Qi"},"source":["# import libs & tools\n","import nltk\n","import glob\n","import re\n","import os\n","import numpy as np\n","import sys\n","\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer, PorterStemmer\n","from nltk.tokenize import sent_tokenize , word_tokenize\n","\n","Stopwords = set(stopwords.words('english'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XAfs0RjDQEfC"},"source":["# support function\n","def finding_all_unique_words_and_freq(words):\n","    words_unique = []\n","    word_freq = {}\n","    for word in words:\n","        if word not in words_unique:\n","            words_unique.append(word)\n","    for word in words_unique:\n","        word_freq[word] = words.count(word)\n","    return word_freq\n","def finding_freq_of_word_in_doc(word,words):\n","    freq = words.count(word)\n","        \n","def remove_special_characters(text):\n","    regex = re.compile('[^a-zA-Z0-9\\s]')\n","    text_returned = re.sub(regex,'',text)\n","    return text_returned"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o_qQAU7MQN5I"},"source":["class Node:\n","    def __init__(self ,docId, freq = None):\n","        self.freq = freq\n","        self.doc = docId\n","        self.nextval = None\n","    \n","class SlinkedList:\n","    def __init__(self ,head = None):\n","        self.head = head"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rNjcmmYjP95k"},"source":["# load data function\n","def Load_data(file_folder):\n","    dict_global = {}\n","    idx = 1\n","    files_with_index = {}\n","    for file in glob.glob(file_folder):\n","        print(file)\n","        fname = file\n","        file = open(file , \"r\")\n","        text = file.read()\n","        text = remove_special_characters(text)\n","        text = re.sub(re.compile('\\d'),'',text)\n","        sentences = sent_tokenize(text)\n","        words = word_tokenize(text)\n","        words = [word for word in words if len(words)>1]\n","        words = [word.lower() for word in words]\n","        words = [word for word in words if word not in Stopwords]\n","        dict_global.update(finding_all_unique_words_and_freq(words))\n","        files_with_index[idx] = os.path.basename(fname)\n","        idx = idx + 1\n","        \n","    unique_words_all = set(dict_global.keys())\n","    return unique_words_all, files_with_index"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GrY4ma7EQSDD"},"source":["# Create linklist\n","def Linklist(unique_words_all, file_folder):\n","    linked_list_data = {}\n","    for word in unique_words_all:\n","        linked_list_data[word] = SlinkedList()\n","        linked_list_data[word].head = Node(1,Node)\n","    word_freq_in_doc = {}\n","    idx = 1\n","    for file in glob.glob(file_folder):\n","        file = open(file, \"r\")\n","        text = file.read()\n","        text = remove_special_characters(text)\n","        text = re.sub(re.compile('\\d'),'',text)\n","        sentences = sent_tokenize(text)\n","        words = word_tokenize(text)\n","        words = [word for word in words if len(words)>1]\n","        words = [word.lower() for word in words]\n","        words = [word for word in words if word not in Stopwords]\n","        word_freq_in_doc = finding_all_unique_words_and_freq(words)\n","        for word in word_freq_in_doc.keys():\n","            linked_list = linked_list_data[word].head\n","            while linked_list.nextval is not None:\n","                linked_list = linked_list.nextval\n","            linked_list.nextval = Node(idx ,word_freq_in_doc[word])\n","        idx = idx + 1\n","    return linked_list_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U8aiqOM9QYF2"},"source":["# main function\n","def Retrieval(files_with_index, linked_list_data):\n","    query = input('Enter your query:')\n","    query = word_tokenize(query)\n","    # print(query)\n","    connecting_words = []\n","    cnt = 1\n","    different_words = []\n","    for word in query:\n","        if word.lower() != \"and\" and word.lower() != \"or\" and word.lower() != \"not\":\n","            different_words.append(word.lower())\n","        else:\n","            connecting_words.append(word.lower())\n","    total_files = len(files_with_index)\n","    zeroes_and_ones = []\n","    zeroes_and_ones_of_all_words = []\n","    for word in (different_words):\n","        if word.lower() in unique_words_all:\n","            zeroes_and_ones = [0] * total_files\n","            linkedlist = linked_list_data[word].head\n","            # print(word)\n","            while linkedlist.nextval is not None:\n","                zeroes_and_ones[linkedlist.nextval.doc - 1] = 1\n","                linkedlist = linkedlist.nextval\n","            zeroes_and_ones_of_all_words.append(zeroes_and_ones)\n","        else:\n","            # print(word,\" not found\")\n","            sys.exit\n","    # print(zeroes_and_ones_of_all_words)\n","    for word in connecting_words:\n","        word_list1 = zeroes_and_ones_of_all_words[0]\n","        word_list2 = zeroes_and_ones_of_all_words[1]\n","        if word == \"and\":\n","            bitwise_op = [w1 & w2 for (w1,w2) in zip(word_list1,word_list2)]\n","            zeroes_and_ones_of_all_words.remove(word_list1)\n","            zeroes_and_ones_of_all_words.remove(word_list2)\n","            zeroes_and_ones_of_all_words.insert(0, bitwise_op);\n","        elif word == \"or\":\n","            bitwise_op = [w1 | w2 for (w1,w2) in zip(word_list1,word_list2)]\n","            zeroes_and_ones_of_all_words.remove(word_list1)\n","            zeroes_and_ones_of_all_words.remove(word_list2)\n","            zeroes_and_ones_of_all_words.insert(0, bitwise_op);\n","        elif word == \"not\":\n","            bitwise_op = [not w1 for w1 in word_list2]\n","            bitwise_op = [int(b == True) for b in bitwise_op]\n","            zeroes_and_ones_of_all_words.remove(word_list2)\n","            zeroes_and_ones_of_all_words.remove(word_list1)\n","            bitwise_op = [w1 & w2 for (w1,w2) in zip(word_list1,bitwise_op)]\n","    zeroes_and_ones_of_all_words.insert(0, bitwise_op);\n","            \n","    files = []    \n","    # print(zeroes_and_ones_of_all_words)\n","    lis = zeroes_and_ones_of_all_words[0]\n","    cnt = 1\n","    for index in lis:\n","        if index == 1:\n","            files.append(files_with_index[cnt])\n","        cnt = cnt+1\n","        \n","    return files"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q0LK_O-vfy_g","executionInfo":{"status":"ok","timestamp":1619949292280,"user_tz":-420,"elapsed":1624,"user":{"displayName":"Hieu Nguyen Quang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhoqAOJz5TvatPuhnWRDsscEFEBB0kylHyhpfL7=s64","userId":"03779775999485249683"}},"outputId":"117b5081-b263-4e8b-e3a9-f656ae861564"},"source":["# Load model\n","unique_words_all, files_with_index = Load_data('data_summaried/*')\n","linked_list_data = Linklist(unique_words_all, 'data_summaried/*')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["data_summaried/football.txt\n","data_summaried/australia.txt\n","data_summaried/text1.txt\n","data_summaried/narendra_modi.txt\n","data_summaried/sneaker.txt\n","data_summaried/cricket.txt\n","data_summaried/india.txt\n","data_summaried/rahul_gandhi.txt\n","data_summaried/apple.txt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rJ2wNS40eclr","executionInfo":{"status":"ok","timestamp":1619949300398,"user_tz":-420,"elapsed":6285,"user":{"displayName":"Hieu Nguyen Quang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhoqAOJz5TvatPuhnWRDsscEFEBB0kylHyhpfL7=s64","userId":"03779775999485249683"}},"outputId":"449fee56-d413-44ce-896b-7d5583cba47e"},"source":["# run\n","files_recommend = Retrieval(files_with_index, linked_list_data)\n","print(files_recommend)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Enter your query:It is the largest country in Oceania and the world's sixth-largest country by total area.\n","['australia.txt']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vDylsoj-fX1z","executionInfo":{"status":"ok","timestamp":1619949309498,"user_tz":-420,"elapsed":5107,"user":{"displayName":"Hieu Nguyen Quang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhoqAOJz5TvatPuhnWRDsscEFEBB0kylHyhpfL7=s64","userId":"03779775999485249683"}},"outputId":"b2489774-38d7-45a0-c86b-a96c43dd2d5a"},"source":["# run\n","files_recommend = Retrieval(files_with_index, linked_list_data)\n","print(files_recommend)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Enter your query:shoes primarily designed for sports or other forms of physical exercise\n","['sneaker.txt']\n"],"name":"stdout"}]}]}